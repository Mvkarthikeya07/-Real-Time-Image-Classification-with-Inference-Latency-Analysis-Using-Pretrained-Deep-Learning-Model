# ðŸš€ Real-Time Image Classification with Inference Latency Analysis Using Pretrained Deep Learning Models

## ðŸ“Œ Project Overview

This project demonstrates a **real-time image classification system** with a strong emphasis on **neural network inferencing performance**. A user uploads an image through a web interface, the system classifies the image using a **pretrained deep learning model**, and displays both the **predicted class** and the **time taken (latency)** to generate the prediction.

The core objective of this project is **not model training**, but **efficient inference and performance evaluation**, which is a critical requirement in real-world AI systems.

---

## ðŸŽ¯ Key Objectives

* Perform **image classification** using a pretrained deep learning model
* Measure **real-time inference latency** in milliseconds
* Demonstrate **optimized inferencing concepts** used in production AI systems
* Provide a **clean, app-like user interface** for interaction
* Bridge the gap between **AI theory and AI engineering**

---

## ðŸ§  System Workflow

1. User uploads an image via the web interface
2. Image is preprocessed (resize, normalization)
3. Pretrained **ResNet-50** model performs inference
4. The system predicts the object class from **ImageNet (1000 classes)**
5. Inference time (latency) is calculated
6. Result is displayed with:

   * Uploaded image preview
   * Predicted class
   * Confidence score
   * Inference latency

---

## âš¡ Role of Optimization Technologies

Although inference is demonstrated locally using PyTorch, this project is conceptually designed around **industry-grade optimization tools**:

* **TensorRT** â€“ Optimizes trained neural networks for high-speed inference on GPUs
* **Triton Inference Server** â€“ Enables scalable and efficient deployment of optimized models

These technologies significantly reduce inference latency in production environments and are widely used in real-time AI applications.

---

## ðŸ› ï¸ Tech Stack

* **Programming Language:** Python
* **Deep Learning Framework:** PyTorch
* **Model:** ResNet-50 (ImageNet pretrained)
* **Web Framework:** Flask
* **Frontend:** HTML, CSS
* **Image Processing:** Pillow / Torchvision

---

## ðŸ“Š Sample Results

* **Predicted Class:** Egyptian Cat / Beagle (ImageNet classes)
* **Confidence:** ~65â€“80%
* **Inference Latency:** ~150â€“450 ms (CPU-based inference)

> âš ï¸ Latency is higher on CPU. With GPU + TensorRT optimization, latency can reduce to **single-digit milliseconds**.

---

## ðŸ–¼ï¸ Application Screenshots

### ðŸ”¹ Home Screen

![Home Screen](Screenshot%20\(106\).png)

### ðŸ”¹ Image Classification Result â€“ Cat

![Cat Prediction](Screenshot%20\(107\).png)

### ðŸ”¹ Image Classification Result â€“ Dog

![Dog Prediction](Screenshot%20\(108\).png)

---

## ðŸŽ“ Academic Significance

This project highlights:

* Difference between **training and inference**
* Importance of **latency in real-world AI systems**
* Practical understanding of **AI deployment and optimization**
* Engineering-oriented thinking beyond basic ML models

Such concepts are highly valued in **research-focused and industry-aligned AI programs**.

---

## ðŸ Conclusion

The project successfully demonstrates an **optimized image classification pipeline** with real-time latency analysis. It emphasizes the role of inference optimization in scalable AI systems and provides hands-on exposure to performance-aware AI engineering.

---

## ðŸ“Œ Future Enhancements

* GPU-based TensorRT optimization
* Triton Inference Server deployment
* Top-5 prediction visualization
* Batch inference performance analysis

---

## ðŸ‘¤ Author

**M V Karthikeya**

---

> *This project focuses on AI system performance and inference optimization rather than model training or dataset creation.*
